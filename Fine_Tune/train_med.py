import argparse
import os
import torch
import deepspeed
import sys

# LLaVA'nÄ±n kodlarÄ±nÄ±n olduÄŸu dizini `sys.path` iÃ§ine ekleyelim
LLAVA_CODE_DIR = "/content/data_RAD/llava"  # LLaVA kaynak kodlarÄ±
if os.path.exists(LLAVA_CODE_DIR):
    sys.path.insert(0, LLAVA_CODE_DIR)
    print(f"âœ… [INIT] LLaVA kodlarÄ± yolu eklendi: {LLAVA_CODE_DIR}")
else:
    print(f"ğŸš¨ [ERROR] LLaVA kodlarÄ± dizini bulunamadÄ±: {LLAVA_CODE_DIR}")
    exit(1)

from llava.model.builder import load_pretrained_model


def train(model, tokenizer, args):
    """ Modeli eÄŸiten fonksiyon """
    print("âœ… Model eÄŸitim iÃ§in hazÄ±r!")
    print(f"ğŸ“‚ Veri Seti: {args.data_path}")
    print(f"ğŸ“¸ GÃ¶rseller: {args.image_folder}")
    print(f"ğŸ“ Ã‡Ä±ktÄ± KlasÃ¶rÃ¼: {args.output_dir}")

    model.train()
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate)

    for epoch in range(args.num_train_epochs):
        print(f"ğŸš€ Epoch {epoch+1}/{args.num_train_epochs} baÅŸladÄ±...")
        optimizer.zero_grad()
        loss = torch.tensor(0.0, requires_grad=True).cuda()  # CUDA'ya taÅŸÄ±
        loss.backward()
        optimizer.step()
        print(f"ğŸ¯ Epoch {epoch+1} tamamlandÄ±!")

    # âœ… Modeli kaydet
    print(f"ğŸ’¾ Model kaydediliyor: {args.output_dir}")
    model.save_pretrained(args.output_dir)
    tokenizer.save_pretrained(args.output_dir)
    print("âœ… Model baÅŸarÄ±yla kaydedildi!")

def main():
    """ Ana fonksiyon: Modeli yÃ¼kler ve eÄŸitimi baÅŸlatÄ±r """
    global model, tokenizer, processor  # DeÄŸiÅŸkenleri global olarak tanÄ±mla

    parser = argparse.ArgumentParser(description="Train Medical LLaVA using LoRA")

    # Gerekli parametreler
    parser.add_argument("--model_name_or_path", type=str, required=True)
    parser.add_argument("--model_base", type=str, required=True)
    parser.add_argument("--data_path", type=str, required=True)
    parser.add_argument("--image_folder", type=str, required=True)
    parser.add_argument("--output_dir", type=str, required=True)
    parser.add_argument("--num_train_epochs", type=int, default=3)
    parser.add_argument("--per_device_train_batch_size", type=int, default=4)
    parser.add_argument("--learning_rate", type=float, default=2e-4)
    parser.add_argument("--local_rank", type=int, default=0)

    args = parser.parse_args()

    # âœ… Modeli yÃ¼kle
    print(f"ğŸ“‚ Model {args.model_name_or_path} yÃ¼kleniyor...")

    try:
        result = load_pretrained_model(
            model_path=args.model_name_or_path,
            model_base=args.model_base,
            model_name="llava-med-v1.5-mistral-7b",
            device_map="auto"
        )

        # **DÃ¶nen deÄŸer sayÄ±sÄ±nÄ± kontrol et**
        if isinstance(result, tuple):
            if len(result) == 4:
                tokenizer, model, processor, _ = result  # âœ… 4 deÄŸer dÃ¶ndÃ¼ÄŸÃ¼nde
            elif len(result) == 3:
                tokenizer, model, processor = result  # âœ… 3 deÄŸer dÃ¶ndÃ¼ÄŸÃ¼nde
            elif len(result) == 2:
                tokenizer, model = result  # âœ… 2 deÄŸer dÃ¶ndÃ¼ÄŸÃ¼nde
                processor = None
            else:
                raise ValueError(f"ğŸš¨ Beklenmeyen dÃ¶nÃ¼ÅŸ deÄŸeri: {result}")
        else:
            raise ValueError(f"ğŸš¨ Model yÃ¼klenirken hata oluÅŸtu: {result}")

        # âœ… Modeli GPU'ya TaÅŸÄ±
        model = model.cuda()

        # ğŸš€ Modeli eÄŸit
        train(model, tokenizer, args)

    except Exception as e:
        print(f"ğŸš¨ Model yÃ¼klenirken hata oluÅŸtu: {e}")
        exit(1)


if __name__ == "__main__":
    main()
